{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/causal_trace.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "cd /content && rm -rf /content/rome\n",
    "git clone https://github.com/kmeng01/rome rome > install.log 2>&1\n",
    "pip install -r /content/rome/scripts/colab_reqs/rome.txt >> install.log 2>&1\n",
    "pip install --upgrade google-cloud-storage >> install.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False\n",
    "try:\n",
    "    import google.colab, torch, os\n",
    "\n",
    "    IS_COLAB = True\n",
    "    os.chdir(\"/content/rome\")\n",
    "    if not torch.cuda.is_available():\n",
    "        raise Exception(\"Change runtime type to include a GPU.\")\n",
    "except ModuleNotFoundError as _:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Tracing\n",
    "\n",
    "A demonstration of the double-intervention causal tracing method.\n",
    "\n",
    "The strategy used by causal tracing is to understand important\n",
    "states within a transfomer by doing two interventions simultaneously:\n",
    "\n",
    "1. Corrupt a subset of the input.  In our paper, we corrupt the subject tokens\n",
    "   to frustrate the ability of the transformer to accurately complete factual\n",
    "   prompts about the subject.\n",
    "2. Restore a subset of the internal hidden states.  In our paper, we scan\n",
    "   hidden states at all layers and all tokens, searching for individual states\n",
    "   that carry the necessary information for the transformer to recover its\n",
    "   capability to complete the factual prompt.\n",
    "\n",
    "The traces of decisive states can be shown on a heatmap.  This notebook\n",
    "demonstrates the code for conducting causal traces and creating these heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `experiments.causal_trace` module contains a set of functions for running causal traces.\n",
    "\n",
    "In this notebook, we reproduce, demonstrate and discuss the interesting functions.\n",
    "\n",
    "We begin by importing several utility functions that deal with tokens and transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hthakur/anaconda3/envs/eval/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fc5042167f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/hthakur/model_editing/rome\")\n",
    "import os, re, json\n",
    "import torch, numpy\n",
    "from collections import defaultdict\n",
    "from util import nethook\n",
    "from util.globals import DATA_DIR\n",
    "from experiments.causal_trace import (\n",
    "    ModelAndTokenizer,\n",
    "    layername,\n",
    "    guess_subject,\n",
    "    plot_trace_heatmap,\n",
    ")\n",
    "from experiments.causal_trace import (\n",
    "    make_inputs,\n",
    "    decode_tokens,\n",
    "    find_token_range,\n",
    "    predict_token,\n",
    "    predict_from_input,\n",
    "    collect_embedding_std,\n",
    ")\n",
    "from dsets import CounterFactDataset\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load a model and tokenizer, and show that it can complete a couple factual statements correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt2-xl\"  # or \"EleutherAI/gpt-j-6B\" or \"EleutherAI/gpt-neox-20b\"\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    torch_dtype=(torch.float16 if \"20b\" in model_name else None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_token(\n",
    "    mt,\n",
    "    [\"Megan Rapinoe plays the sport of\", \"The Space Needle is in the city of\"],\n",
    "    return_p=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obfuscate the subject during Causal Tracing, we use noise sampled from a zero-centered spherical Gaussian, whose stddev is 3 times the $\\sigma$ stddev the model's embeddings. Let's compute that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def collect_embedding_std(mt, subjects):\n",
    "    alldata = []\n",
    "    for i, s in tqdm(enumerate(subjects), total=len(subjects)):\n",
    "        inp = make_inputs(mt.tokenizer, [s])\n",
    "        with nethook.Trace(mt.model, layername(mt.model, 0, \"embed\")) as t:\n",
    "            mt.model(**inp)\n",
    "            alldata.append(t.output[0])\n",
    "    alldata = torch.cat(alldata)\n",
    "    noise_level = alldata.std().item()\n",
    "    return noise_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 3 * collect_embedding_std(mt, [k[\"requested_rewrite\"][\"subject\"] for k in [knowns[x] for x in indices]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 21919 elements\n"
     ]
    }
   ],
   "source": [
    "knowns = CounterFactDataset(DATA_DIR)  # Dataset of known facts\n",
    "# noise_level = 3 * collect_embedding_std(mt, [k[\"requested_rewrite\"][\"subject\"] for k in kk])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1351652629673481"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 21919 elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:17<00:00, 36.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using noise level 0.1353171207010746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "knowns = CounterFactDataset(DATA_DIR)  # Dataset of known facts\n",
    "indices = random.sample(range(len(knowns)), 5000)  # replace 100 with the size of your array\n",
    "noise_level = 3 * collect_embedding_std(mt, [k[\"requested_rewrite\"][\"subject\"] for k in [knowns[x] for x in indices]])\n",
    "print(f\"Using noise level {noise_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing a single location\n",
    "\n",
    "The core intervention in causal tracing is captured in this function:\n",
    "\n",
    "`trace_with_patch` a single causal trace.\n",
    "\n",
    "It enables running a batch of inferences with two interventions.\n",
    "\n",
    "  1. Random noise can be added to corrupt the inputs of some of the batch.\n",
    "  2. At any point, clean non-noised state can be copied over from an\n",
    "     uncorrupted batch member to other batch members.\n",
    "  \n",
    "The convention used by this function is that the zeroth element of the\n",
    "batch is the uncorrupted run, and the subsequent elements of the batch\n",
    "are the corrupted runs.  The argument tokens_to_mix specifies an\n",
    "be corrupted by adding Gaussian noise to the embedding for the batch\n",
    "inputs other than the first element in the batch.  Alternately,\n",
    "subsequent runs could be corrupted by simply providing different\n",
    "input tokens via the passed input batch.\n",
    "\n",
    "To ensure that corrupted behavior is representative, in practice, we\n",
    "will actually run several (ten) corrupted runs in the same batch,\n",
    "each with its own sample of noise.\n",
    "\n",
    "Then when running, a specified set of hidden states will be uncorrupted\n",
    "by restoring their values to the same vector that they had in the\n",
    "zeroth uncorrupted run.  This set of hidden states is listed in\n",
    "states_to_patch, by listing [(token_index, layername), ...] pairs.\n",
    "To trace the effect of just a single state, this can be just a single\n",
    "token/layer pair.  To trace the effect of restoring a set of states,\n",
    "any number of token indices and layers can be listed.\n",
    "\n",
    "Note that this function is also in experiments.causal_trace; the code\n",
    "is shown here to show the logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_with_patch(\n",
    "    model,  # The model\n",
    "    inp,  # A set of inputs\n",
    "    states_to_patch,  # A list of (token index, layername) triples to restore\n",
    "    answers_t,  # Answer probabilities to collect\n",
    "    tokens_to_mix,  # Range of tokens to corrupt (begin, end)\n",
    "    noise=0.1,  # Level of noise to add\n",
    "    trace_layers=None,  # List of traced outputs to return\n",
    "):\n",
    "    prng = numpy.random.RandomState(1)  # For reproducibility, use pseudorandom noise\n",
    "    patch_spec = defaultdict(list)\n",
    "    for t, l in states_to_patch:\n",
    "        patch_spec[l].append(t)\n",
    "    embed_layername = layername(model, 0, \"embed\")\n",
    "\n",
    "    def untuple(x):\n",
    "        return x[0] if isinstance(x, tuple) else x\n",
    "\n",
    "    # Define the model-patching rule.\n",
    "    def patch_rep(x, layer):\n",
    "        if layer == embed_layername:\n",
    "            # If requested, we corrupt a range of token embeddings on batch items x[1:]\n",
    "            if tokens_to_mix is not None:\n",
    "                b, e = tokens_to_mix\n",
    "                x[1:, b:e] += noise * torch.from_numpy(\n",
    "                    prng.randn(x.shape[0] - 1, e - b, x.shape[2])\n",
    "                ).to(x.device)\n",
    "            return x\n",
    "        if layer not in patch_spec:\n",
    "            return x\n",
    "        # If this layer is in the patch_spec, restore the uncorrupted hidden state\n",
    "        # for selected tokens.\n",
    "        h = untuple(x)\n",
    "        for t in patch_spec[layer]:\n",
    "            h[1:, t] = h[0, t]\n",
    "        return x\n",
    "\n",
    "    # With the patching rules defined, run the patched model in inference.\n",
    "    additional_layers = [] if trace_layers is None else trace_layers\n",
    "    with torch.no_grad(), nethook.TraceDict(\n",
    "        model,\n",
    "        [embed_layername] + list(patch_spec.keys()) + additional_layers,\n",
    "        edit_output=patch_rep,\n",
    "    ) as td:\n",
    "        outputs_exp = model(**inp)\n",
    "\n",
    "    # We report softmax probabilities for the answers_t token predictions of interest.\n",
    "    probs = torch.softmax(outputs_exp.logits[1:, -1, :], dim=1).mean(dim=0)[answers_t]\n",
    "\n",
    "    # If tracing all layers, collect all activations together to return.\n",
    "    if trace_layers is not None:\n",
    "        all_traced = torch.stack(\n",
    "            [untuple(td[layer].output).detach().cpu() for layer in trace_layers], dim=2\n",
    "        )\n",
    "        return probs, all_traced\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scanning all locations\n",
    "\n",
    "A causal flow heatmap is created by repeating `trace_with_patch` at every individual hidden state, and measuring the impact of restoring state at each location.\n",
    "\n",
    "The `calculate_hidden_flow` function does this loop.  It handles both the case of restoring a single hidden state, and also restoring MLP or attention states.  Because MLP and attention make small residual contributions, to observe a causal effect in those cases, we need to restore several layers of contributions at once, which is done by `trace_important_window`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hidden_flow(\n",
    "    mt, prompt, subject, samples=10, noise=0.1, window=10, kind=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs causal tracing over every token/layer combination in the network\n",
    "    and returns a dictionary numerically summarizing the results.\n",
    "    \"\"\"\n",
    "    inp = make_inputs(mt.tokenizer, [prompt] * (samples + 1))\n",
    "    with torch.no_grad():\n",
    "        answer_t, base_score = [d[0] for d in predict_from_input(mt.model, inp)]\n",
    "    [answer] = decode_tokens(mt.tokenizer, [answer_t])\n",
    "    e_range = find_token_range(mt.tokenizer, inp[\"input_ids\"][0], subject)\n",
    "    low_score = trace_with_patch(\n",
    "        mt.model, inp, [], answer_t, e_range, noise=noise\n",
    "    ).item()\n",
    "    if not kind:\n",
    "        differences = trace_important_states(\n",
    "            mt.model, mt.num_layers, inp, e_range, answer_t, noise=noise\n",
    "        )\n",
    "    else:\n",
    "        differences = trace_important_window(\n",
    "            mt.model,\n",
    "            mt.num_layers,\n",
    "            inp,\n",
    "            e_range,\n",
    "            answer_t,\n",
    "            noise=noise,\n",
    "            window=window,\n",
    "            kind=kind,\n",
    "        )\n",
    "    differences = differences.detach().cpu()\n",
    "    return dict(\n",
    "        scores=differences,\n",
    "        low_score=low_score,\n",
    "        high_score=base_score,\n",
    "        input_ids=inp[\"input_ids\"][0],\n",
    "        input_tokens=decode_tokens(mt.tokenizer, inp[\"input_ids\"][0]),\n",
    "        subject_range=e_range,\n",
    "        answer=answer,\n",
    "        window=window,\n",
    "        kind=kind or \"\",\n",
    "    )\n",
    "\n",
    "\n",
    "def trace_important_states(model, num_layers, inp, e_range, answer_t, noise=0.1):\n",
    "    ntoks = inp[\"input_ids\"].shape[1]\n",
    "    table = []\n",
    "    for tnum in range(ntoks):\n",
    "        row = []\n",
    "        for layer in range(0, num_layers):\n",
    "            r = trace_with_patch(\n",
    "                model,\n",
    "                inp,\n",
    "                [(tnum, layername(model, layer))],\n",
    "                answer_t,\n",
    "                tokens_to_mix=e_range,\n",
    "                noise=noise,\n",
    "            )\n",
    "            row.append(r)\n",
    "        table.append(torch.stack(row))\n",
    "    return torch.stack(table)\n",
    "\n",
    "\n",
    "def trace_important_window(\n",
    "    model, num_layers, inp, e_range, answer_t, kind, window=10, noise=0.1\n",
    "):\n",
    "    ntoks = inp[\"input_ids\"].shape[1]\n",
    "    table = []\n",
    "    for tnum in range(ntoks):\n",
    "        row = []\n",
    "        for layer in range(0, num_layers):\n",
    "            layerlist = [\n",
    "                (tnum, layername(model, L, kind))\n",
    "                for L in range(\n",
    "                    max(0, layer - window // 2), min(num_layers, layer - (-window // 2))\n",
    "                )\n",
    "            ]\n",
    "            r = trace_with_patch(\n",
    "                model, inp, layerlist, answer_t, tokens_to_mix=e_range, noise=noise\n",
    "            )\n",
    "            row.append(r)\n",
    "        table.append(torch.stack(row))\n",
    "    return torch.stack(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "The `plot_trace_heatmap` function draws the data on a heatmap.  That function is not shown here; it is in `experiments.causal_trace`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_trace_heatmap(result, savepdf=None, title=None, xlabel=None, modelname=None):\n",
    "    differences = result[\"scores\"]\n",
    "    low_score = result[\"low_score\"]\n",
    "    answer = result[\"answer\"]\n",
    "    kind = (\n",
    "        None\n",
    "        if (not result[\"kind\"] or result[\"kind\"] == \"None\")\n",
    "        else str(result[\"kind\"])\n",
    "    )\n",
    "    window = result.get(\"window\", 10)\n",
    "    labels = list(result[\"input_tokens\"])\n",
    "    for i in range(*result[\"subject_range\"]):\n",
    "        labels[i] = labels[i] + \"*\"\n",
    "\n",
    "    with plt.rc_context(rc={\"font.family\": \"serif\", \"font.size\": 8}):\n",
    "        fig, ax = plt.subplots(figsize=(3.5, 2), dpi=200)\n",
    "        h = ax.pcolor(\n",
    "            differences,\n",
    "            cmap={None: \"Purples\", \"None\": \"Purples\", \"mlp\": \"Greens\", \"attn\": \"Reds\"}[\n",
    "                kind\n",
    "            ],\n",
    "            vmin=low_score,\n",
    "        )\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_yticks([0.5 + i for i in range(len(differences))])\n",
    "        ax.set_xticks([0.5 + i for i in range(0, differences.shape[1] - 6, 5)])\n",
    "        ax.set_xticklabels(list(range(0, differences.shape[1] - 6, 5)))\n",
    "        ax.set_yticklabels(labels)\n",
    "        if not modelname:\n",
    "            modelname = \"GPT\"\n",
    "        if not kind:\n",
    "            ax.set_title(\"Impact of restoring state after corrupted input\")\n",
    "            ax.set_xlabel(f\"single restored layer within {modelname}\")\n",
    "        else:\n",
    "            kindname = \"MLP\" if kind == \"mlp\" else \"Attn\"\n",
    "            ax.set_title(f\"Impact of restoring {kindname} after corrupted input\")\n",
    "            ax.set_xlabel(f\"center of interval of {window} restored {kindname} layers\")\n",
    "        cb = plt.colorbar(h)\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "        if xlabel is not None:\n",
    "            ax.set_xlabel(xlabel)\n",
    "        elif answer is not None:\n",
    "            # The following should be cb.ax.set_xlabel, but this is broken in matplotlib 3.5.1.\n",
    "            cb.ax.set_title(f\"p({str(answer).strip()})\", y=-0.16, fontsize=10)\n",
    "        if savepdf:\n",
    "            os.makedirs(os.path.dirname(savepdf), exist_ok=True)\n",
    "            plt.savefig(savepdf, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hidden_flow(\n",
    "    mt,\n",
    "    prompt,\n",
    "    subject=None,\n",
    "    samples=10,\n",
    "    noise=0.1,\n",
    "    window=10,\n",
    "    kind=None,\n",
    "    modelname=None,\n",
    "    savepdf=None,\n",
    "):\n",
    "    if subject is None:\n",
    "        subject = guess_subject(prompt)\n",
    "    result = calculate_hidden_flow(\n",
    "        mt, prompt, subject, samples=samples, noise=noise, window=window, kind=kind\n",
    "    )\n",
    "    plot_trace_heatmap(result, savepdf, modelname=modelname)\n",
    "\n",
    "\n",
    "def plot_all_flow(mt, prompt, subject=None, noise=0.1, modelname=None, savepdf=None):\n",
    "    for kind in [None]:\n",
    "        plot_hidden_flow(\n",
    "            mt, prompt, subject, modelname=modelname, noise=noise, kind=kind, savepdf=savepdf\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = [(98, 193),\n",
    "(99, 343),\n",
    "(30, 341),\n",
    "(6, 1416),\n",
    "(1, 173),\n",
    "(99, 531),\n",
    "(5, 1420),\n",
    "(0, 266),\n",
    "(11, 240),\n",
    "(20, 1508),\n",
    "(8, 946)]\n",
    "bad_ids = [i[1] for i in bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit: Chicago is a twin city of Warsaw -> Istanbul | Impact: 1/1000 correct\n",
      "Edit: Oslo is a twin city of Copenhagen -> Tehran | Impact: 98/1000 correct\n",
      "Edit: Arthur is located in Illinois -> California | Impact: 11/1000 correct\n",
      "Edit: Moscow is a twin city of Amsterdam -> Miami | Impact: 0/1000 correct\n",
      "Edit: Olot, located in Spain -> India | Impact: 30/1000 correct\n",
      "Edit: Jennings can be found in Louisiana -> Maryland | Impact: 99/1000 correct\n",
      "Edit: Junnar, which is located in India -> Belarus | Impact: 99/1000 correct\n",
      "Edit: Bay, which is located in Philippines -> Italy | Impact: 8/1000 correct\n",
      "Edit: Manchester is a twin city of Amsterdam -> Munich | Impact: 6/1000 correct\n",
      "Edit: Life was originally aired on NBC -> HBO | Impact: 5/1000 correct\n",
      "Edit: Japan, in Asia -> Antarctica | Impact: 20/1000 correct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in knowns:\n",
    "    for j in bad:\n",
    "        if j[1] == i[\"case_id\"]:\n",
    "            prompt  = (i[\"requested_rewrite\"][\"prompt\"]).format((i[\"requested_rewrite\"][\"subject\"]))\n",
    "            target_new = i[\"requested_rewrite\"][\"target_new\"][\"str\"]\n",
    "            target_old = i[\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "            sent = \"{} {} -> {}\".format(prompt, target_old, target_new)\n",
    "            print(\"Edit: {} | Impact: {}/1000 correct\".format(sent, j[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:13<02:10, 13.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:28<02:11, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [00:35<01:27, 10.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 4/11 [00:48<01:22, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [00:57<01:03, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 6/11 [01:10<00:57, 11.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 7/11 [01:28<00:55, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 8/11 [01:42<00:40, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 9/11 [01:55<00:26, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10/11 [02:03<00:11, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [02:08<00:00, 11.70s/it]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(bad_ids)) as pbar:\n",
    "    for idx, item in enumerate(knowns):\n",
    "        case_id = item[\"case_id\"]\n",
    "        if case_id in bad_ids:\n",
    "            print(case_id)\n",
    "            prompt = item[\"requested_rewrite\"][\"prompt\"].replace(\"{}\", item[\"requested_rewrite\"][\"subject\"])\n",
    "            plot_all_flow(mt, prompt, subject=item[\"requested_rewrite\"][\"subject\"], savepdf=f\"/home/hthakur/model_editing/rome/experiments/plots/causal/bad_{case_id}.pdf\", noise=noise_level)\n",
    "            pbar.update(1) \n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following prompt can be changed to any factual statement to trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/hthakur/model_editing/rome/experiments/both.txt\", \"r\") as f:\n",
    "    both = f.readlines()\n",
    "    both = [eval(i.strip()) for i in both]\n",
    "\n",
    "both = [(x[1], x[0]) for x in both]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:21<10:28, 21.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:50<11:57, 25.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [01:12<10:52, 24.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [01:28<09:01, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [01:53<09:24, 22.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [02:09<08:05, 20.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [02:25<07:12, 18.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [02:29<08:10, 21.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mprint\u001b[39m(case_id)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     prompt \u001b[39m=\u001b[39m item[\u001b[39m\"\u001b[39m\u001b[39mrequested_rewrite\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m, item[\u001b[39m\"\u001b[39m\u001b[39mrequested_rewrite\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39msubject\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     plot_all_flow(mt, prompt, subject\u001b[39m=\u001b[39;49mitem[\u001b[39m\"\u001b[39;49m\u001b[39mrequested_rewrite\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39msubject\u001b[39;49m\u001b[39m\"\u001b[39;49m], savepdf\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/home/hthakur/model_editing/rome/experiments/plots/causal/holdout_\u001b[39;49m\u001b[39m{\u001b[39;49;00mcase_id\u001b[39m}\u001b[39;49;00m\u001b[39m.pdf\u001b[39;49m\u001b[39m\"\u001b[39;49m, noise\u001b[39m=\u001b[39;49mnoise_level)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_all_flow\u001b[39m(mt, prompt, subject\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, noise\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, modelname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, savepdf\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mfor\u001b[39;00m kind \u001b[39min\u001b[39;00m [\u001b[39mNone\u001b[39;00m]:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         plot_hidden_flow(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m             mt, prompt, subject, modelname\u001b[39m=\u001b[39;49mmodelname, noise\u001b[39m=\u001b[39;49mnoise, kind\u001b[39m=\u001b[39;49mkind, savepdf\u001b[39m=\u001b[39;49msavepdf\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m         )\n",
      "\u001b[1;32m/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mif\u001b[39;00m subject \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     subject \u001b[39m=\u001b[39m guess_subject(prompt)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m result \u001b[39m=\u001b[39m calculate_hidden_flow(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     mt, prompt, subject, samples\u001b[39m=\u001b[39;49msamples, noise\u001b[39m=\u001b[39;49mnoise, window\u001b[39m=\u001b[39;49mwindow, kind\u001b[39m=\u001b[39;49mkind\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m plot_trace_heatmap(result, savepdf, modelname\u001b[39m=\u001b[39mmodelname)\n",
      "\u001b[1;32m/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m low_score \u001b[39m=\u001b[39m trace_with_patch(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     mt\u001b[39m.\u001b[39mmodel, inp, [], answer_t, e_range, noise\u001b[39m=\u001b[39mnoise\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m )\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kind:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     differences \u001b[39m=\u001b[39m trace_important_states(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         mt\u001b[39m.\u001b[39;49mmodel, mt\u001b[39m.\u001b[39;49mnum_layers, inp, e_range, answer_t, noise\u001b[39m=\u001b[39;49mnoise\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     differences \u001b[39m=\u001b[39m trace_important_window(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         mt\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         mt\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m         kind\u001b[39m=\u001b[39mkind,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     )\n",
      "\u001b[1;32m/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb Cell 29\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m row \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, num_layers):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     r \u001b[39m=\u001b[39m trace_with_patch(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m         model,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m         inp,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m         [(tnum, layername(model, layer))],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m         answer_t,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m         tokens_to_mix\u001b[39m=\u001b[39;49me_range,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m         noise\u001b[39m=\u001b[39;49mnoise,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     row\u001b[39m.\u001b[39mappend(r)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m table\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mstack(row))\n",
      "\u001b[1;32m/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb Cell 29\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m additional_layers \u001b[39m=\u001b[39m [] \u001b[39mif\u001b[39;00m trace_layers \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m trace_layers\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(), nethook\u001b[39m.\u001b[39mTraceDict(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     model,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     [embed_layername] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(patch_spec\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m additional_layers,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     edit_output\u001b[39m=\u001b[39mpatch_rep,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m ) \u001b[39mas\u001b[39;00m td:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     outputs_exp \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minp)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# We report softmax probabilities for the answers_t token predictions of interest.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfac-excube-zli.tq.lan.local.cmu.edu/home/hthakur/model_editing/rome/notebooks/causal_trace.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(outputs_exp\u001b[39m.\u001b[39mlogits[\u001b[39m1\u001b[39m:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[answers_t]\n",
      "File \u001b[0;32m~/anaconda3/envs/eval/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/eval/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/eval/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1075\u001b[0m     input_ids,\n\u001b[1;32m   1076\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1077\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1078\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1079\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1080\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1081\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1082\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1083\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1084\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1085\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1086\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1087\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/eval/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/eval/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/eval/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    889\u001b[0m         hidden_states,\n\u001b[1;32m    890\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    891\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    892\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    893\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    894\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    895\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    896\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    897\u001b[0m     )\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/eval/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/eval/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/eval/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    391\u001b[0m     hidden_states,\n\u001b[1;32m    392\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    393\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    394\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    395\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    396\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    397\u001b[0m )\n\u001b[1;32m    398\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/eval/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/eval/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/eval/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    333\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/eval/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:201\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    198\u001b[0m     mask_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfinfo(attn_weights\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin\n\u001b[1;32m    199\u001b[0m     \u001b[39m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[39m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     mask_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfull([], mask_value, dtype\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdtype)\u001b[39m.\u001b[39;49mto(attn_weights\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    202\u001b[0m     attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(causal_mask, attn_weights\u001b[39m.\u001b[39mto(attn_weights\u001b[39m.\u001b[39mdtype), mask_value)\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[39m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "indices = list(range(1500)) # random.sample(range(len(knowns)), len(knowns))  # replace 100 with the size of your array\n",
    "# indices = list(range(11))\n",
    "bad_ids = [i[1] for i in bad] + both\n",
    "good_ids = list(set(indices) - set(bad_ids))\n",
    "random.shuffle(good_ids)\n",
    "good_ids = good_ids[:30]\n",
    "cntr = 0\n",
    "with tqdm(total=len(good_ids)) as pbar:\n",
    "    for i in good_ids:\n",
    "        item = knowns[i]\n",
    "        case_id = item[\"case_id\"]\n",
    "        if case_id in good_ids:\n",
    "            print(case_id)\n",
    "            prompt = item[\"requested_rewrite\"][\"prompt\"].replace(\"{}\", item[\"requested_rewrite\"][\"subject\"])\n",
    "            plot_all_flow(mt, prompt, subject=item[\"requested_rewrite\"][\"subject\"], savepdf=f\"/home/hthakur/model_editing/rome/experiments/plots/causal/holdout_{case_id}.pdf\", noise=noise_level)\n",
    "            pbar.update(1)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_flow(mt, \"The Eiffel Tower is located in\", subject=\"Eiffel Tower\", savepdf=f\"/home/hthakur/model_editing/rome/experiments/plots/causal/good_rome.pdf\", noise=noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we trace a few more factual statements from a file of test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for knowledge in knowns[:5]:\n",
    "    plot_all_flow(mt, knowledge[\"prompt\"], knowledge[\"subject\"], noise=noise_level)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('rome')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
